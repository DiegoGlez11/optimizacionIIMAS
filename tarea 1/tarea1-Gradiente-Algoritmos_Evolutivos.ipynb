{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e800a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as ipw\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab00cab",
   "metadata": {},
   "source": [
    "## El gradiente\n",
    "\n",
    "Para calcular el gradiente se hace uso de la liberia pytorch. Cuando se define una función con tensores que requieren el gradiente se crea una gráfica de procesamiento, la cual es usada para representar la función donde los nodo hoja son las variables respecto a las cuales se va a sacar la derivada de la función. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cebdd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función f(x1,x2) = 418.9829*2 - x1*sin(sqrt(abs(x1))) - x2*sin(sqrt(abs(x2)))\n",
    "def f2(x_1,x_2, grad=False):\n",
    "    if grad:\n",
    "        x1 = torch.tensor(float(x_1), requires_grad=True)\n",
    "        x2 = torch.tensor(float(x_2), requires_grad=True)\n",
    "        a = torch.tensor(418.9829*2)\n",
    "        f = a - x1*torch.sin(torch.sqrt(torch.abs(x1))) - x2*torch.sin(torch.sqrt(torch.abs(x2)))\n",
    "        f.backward()\n",
    "        return f, np.array([x1.grad, x2.grad])\n",
    "    else:\n",
    "        x1 = torch.tensor(x_1)\n",
    "        x2 = torch.tensor(x_2)\n",
    "        a = torch.tensor(418.9829*2)\n",
    "        \n",
    "        f = a - x1*torch.sin(torch.sqrt(torch.abs(x1))) - x2*torch.sin(torch.sqrt(torch.abs(x2)))\n",
    "        return f\n",
    "\n",
    "#Función f1(x1,x2) = x1^2+x2^2\n",
    "def f1(x_1,x_2, grad=False):\n",
    "    if grad:\n",
    "        x1 = torch.tensor(float(x_1), requires_grad=True)\n",
    "        x2 = torch.tensor(float(x_2), requires_grad=True)\n",
    "        f = x1**2+x2**2\n",
    "        f.backward()\n",
    "        return f, np.array([x1.grad, x2.grad])\n",
    "    else:\n",
    "        x1 = torch.tensor(x_1)\n",
    "        x2 = torch.tensor(x_2)\n",
    "        f = x1**2+x2**2\n",
    "        return f\n",
    "\n",
    "#Algoritmo de backtracking para obtener el valor óptimo de alpha (tasa de descenso en la función)\n",
    "def get_paso(f,xk,pk, alpha, rho=0.5, c=10**-4):\n",
    "    n=0\n",
    "    x_ = xk + alpha*pk\n",
    "    \n",
    "    while not (f(x_[0], x_[1]) <= f(xk[0], xk[1]) + c*alpha*np.dot((-1*pk),pk)) and n < 1000:\n",
    "        x_ = xk + alpha*pk\n",
    "        alpha = rho*alpha\n",
    "        n+=1\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9db5af",
   "metadata": {},
   "source": [
    "## Algoritmo de gradiente descendente\n",
    "\n",
    "\n",
    "En la siguiente celda se muestra el código del gradiente descendente. Dicho código crea una gráfica interactiva en la cual se observa el comportamiento del gradiente descendente. \n",
    "\n",
    "El menu `f` se elige la función, `f1` es la función `f1(x1,x2) = x1^2+x2^2` y `f2` es `f(x1,x2) = 418.9829*2 - x1*sin(sqrt(abs(x1))) - x2*sin(sqrt(abs(x2)))`.\n",
    "El slider `giro1` y `giro2` se usan para girar la gráfica y poder observar la función desde otro ángulo.\n",
    "Los campos de texto `x0` y `y0` sirven para elegir el punto inicial.\n",
    "El campo `àlpha` es para ingresar la tasa de descenso inicial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ec0923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db3792f494549db8a4439c63944089b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='f', options=('f1', 'f2'), value='f1'), IntSlider(value=-15, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(f=[\"f1\",\"f2\"], giro1=(-360,360,), giro2=(-360,360), x0=\"-300\", y0=\"300\", alpha=\"0.2\")\n",
    "def gradient(f=\"f1\",giro1=-15.0,giro2=38, x0=\"-300\", y0=\"300\", alpha=\"0.2\"):\n",
    "    \n",
    "    rangeX = [-500,500]\n",
    "    rangeY = [-500,500]\n",
    "    n_points=40 #numero de puntos de la grafica n_points*n_points\n",
    "    f_name = f\n",
    "    f = globals()[f] #Funcion a usar\n",
    "    \n",
    "    #número de iteraciones límite\n",
    "    n_iter = 500\n",
    "    n = 0\n",
    "    #alpha inicial\n",
    "    alpha = float(alpha)\n",
    "    \n",
    "    #graph\n",
    "    fig=plt.figure(figsize=(18,6))\n",
    "    axes=fig.add_subplot(111, projection='3d')\n",
    "    axes.view_init(giro1,giro2)\n",
    "    axes.set_title(\"Funcion \",fontsize=14,fontweight=\"bold\")\n",
    "    axes.set_xlabel(\"X\")\n",
    "    axes.set_ylabel(\"Y\")\n",
    "    axes.set_zlabel(\"Z\")\n",
    "    x,y = np.meshgrid(np.linspace(rangeX[0], rangeX[1], n_points), np.linspace(rangeY[0], rangeY[1], n_points))\n",
    "    z=np.array(f(x,y))\n",
    "    \n",
    "    #valor inicial X0\n",
    "    xk = np.array([float(x0), float(y0)])\n",
    "    \n",
    "    history_point = np.empty([0,3])\n",
    "    history_alpha = []\n",
    "    \n",
    "    #Se calcula el valor de la función junto con el gradiente en el punto f(xk)\n",
    "    aux = f(xk[0],xk[1], grad=True)\n",
    "    fk = aux[0] #valor de la función\n",
    "    pk = -1*aux[1] #gradiente\n",
    "    point = np.array([float(xk[0]), float(xk[1]), float(fk)])#punto azul de la gráfica\n",
    "    history_point = np.concatenate((history_point, [point]))\n",
    "\n",
    "    #Gradiente descendente\n",
    "    while n < n_iter and not (pk[0] == 0 and pk[1] == 0):#Condición de paro\n",
    "        #tamaño de paso\n",
    "        alpha = get_paso(f,xk,pk,alpha)\n",
    "        history_alpha.append(alpha)\n",
    "        #Siguiente punto en el gradiente descendente\n",
    "        xk=xk+alpha*pk\n",
    "        \n",
    "        #Se calcula el valor de la función junto con el gradiente en el punto f(xk)\n",
    "        aux = f(xk[0],xk[1], grad=True)\n",
    "        fk = aux[0] #valor de la función\n",
    "        pk = -1*aux[1] #gradiente\n",
    "        point = np.array([float(xk[0]), float(xk[1]), float(fk)])#punto azul de la gráfica\n",
    "        history_point = np.concatenate((history_point, [point]))\n",
    "        \n",
    "        n+=1\n",
    "     \n",
    "    #resultados del gradiente\n",
    "    axes.cla()\n",
    "    plt.title(\"Funcion \"+f_name)\n",
    "    axes.plot_surface(x,y,z, color=(0.1, 0.2, 0.5, 0.3))\n",
    "    axes.scatter3D(history_point[:,0],history_point[:,1], history_point[:,2],s=80 ,color=\"red\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Resultados finales\\nPunto:\",point,\"Gradiente:\",pk,\"Alpha:\",alpha)\n",
    "    \n",
    "    #grafica de alpha\n",
    "    plt.figure(figsize=(13,3))\n",
    "    x = np.arange(1,len(history_alpha)+1)\n",
    "    plt.scatter(x,history_alpha)\n",
    "    plt.title(\"Aplha\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f37f5f",
   "metadata": {},
   "source": [
    "Probando con distintos valores se puede observar que es importante elejir el alpha inicial. Si se elejige un valor de alpha mayor que 0.5 en la función f1() el gradiente descendente salta entre un lado y otro del paraboloide, dirigiéndose al mínimo global pero sin llegar exactamente a él, se usan 500 iteraciones. Si alpha es igual a 0.5, con un solo paso se llega exactamente al mínimo global. Cuando es menor a 0.5 baja sobre una curva hacia el mínimo global, al igual cuando alpha es mayor que 0.5 se acerca el punto del gradiente descendente al mínimo global pero sin llegar exactamente a él.\n",
    "Sin importar cuál sea el punto inicial la función siempre se dirije al mínimo global.\n",
    "\n",
    "La función f2() tiene muchos mínimos locales, el resultado final depende del punto inicial pues el gradiente descendente se dirigíra al mínimo local más cercano. Al igual que en f1() se aproxima al mínimo local y entre más cercano se encuentre el valor de alpha tiende a cero, lo cual suena congruente a los observaciones pues al estar cerca del mínimo la derivada tiende a cero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885e8df",
   "metadata": {},
   "source": [
    "# Algoritmo Genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e258999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from numpy.random import rand\n",
    "from codecs import decode\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c419a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limita los valores de los genes a su rango\n",
    "def constrain(val, bounds):\n",
    "    for i in range(len(val)):\n",
    "        \n",
    "        if val[i] < bounds[i][0]:\n",
    "            val[i] = bounds[i][0]\n",
    "        else:\n",
    "            if val[i] > bounds[i][1]:\n",
    "                val[i] = bounds[i][1]\n",
    "\n",
    "# Selección por torneo binario (cuando k = 2)\n",
    "def selection(pop, scores, k=2):\n",
    "    # Se selecciona aleatoriamente a un individuo\n",
    "    selec_ind = randint(len(pop))\n",
    "    \n",
    "    for selec_ind_2 in randint(0, len(pop), k-1):\n",
    "    \n",
    "        # Se verifica cual individuo tiene un valor mayor en la función fitness\n",
    "        if scores[selec_ind_2] < scores[selec_ind]:\n",
    "            selec_ind = selec_ind_2\n",
    "            \n",
    "    return pop[selec_ind]\n",
    "\n",
    "def crossover_sbx(p1,p2, nc=None):\n",
    "    #nc aleatorio\n",
    "    if nc == None:\n",
    "        nc = randint(0,2,1)\n",
    "        \n",
    "    #número aleatorio\n",
    "    u = np.random.rand()\n",
    "    \n",
    "    #beta\n",
    "    if u <= 0.5:\n",
    "        beta = (2*u)**(1/(nc+1))\n",
    "    else:\n",
    "        beta = (1/(2*(1-u)))**(1/(nc+1))\n",
    "        \n",
    "    #producir hijos\n",
    "    h1 = 0.5*((p1+p2) - beta*abs(p2-p1))\n",
    "    h2 = 0.5*((p1+p2) + beta*abs(p2-p1))\n",
    "    \n",
    "    #hijos en su representación binaria\n",
    "    return h1, h2\n",
    "\n",
    "# mutacion\n",
    "def mutation(p, bounds, n_generacion, nc=1):\n",
    "    nm = 100 + n_generacion\n",
    "    \n",
    "    #número aleatorio\n",
    "    u = np.random.rand()\n",
    "\n",
    "    #delta\n",
    "    if u <= 0.5:\n",
    "        delta = (2*u)**(1/(nc+1)) - 1\n",
    "    else:\n",
    "        delta = 1 - 2*(1-u)**(1/(nc+1))\n",
    "\n",
    "    if u <= 0.5:\n",
    "        deltaQ = (2*u + (1-2*u) + (1-delta)**(nm+1) )**(1/(nm+1)) - 1\n",
    "    else:\n",
    "        deltaQ = 1- (2*(1-u) + 2*(u-0.5)*(1-delta)**(nm+1) )**(1/(nm+1))\n",
    "    \n",
    "    p_ = p+(bounds[:,1]-bounds[:,0])*deltaQ\n",
    "    \n",
    "    constrain(p_, bounds)\n",
    "    \n",
    "    return p_\n",
    "    \n",
    "# genetic algorithm\n",
    "def genetic_algorithm(fitness, bounds, n_bits, n_iter, n_pop):\n",
    "    # población inicial en cadenas binarias\n",
    "    pop = []\n",
    "    for i in range(n_pop):\n",
    "        cromosoma = list()\n",
    "        #se codifican las variables\n",
    "        for lim in bounds:\n",
    "            #número aleatorio dentro de su rango\n",
    "            n = lim[0] +  np.random.rand()*(lim[1]-lim[0])\n",
    "            #se concatena el gen al cromosoma\n",
    "            cromosoma.append(n)\n",
    "        pop.append(cromosoma)\n",
    "    pop = np.array(pop)\n",
    "    \n",
    "    #primer mejor solución\n",
    "    best, best_eval = pop[0], fitness(pop[0][0], pop[0][1])\n",
    "    \n",
    "    #historial de los mejores puntos\n",
    "    history_best_ind = np.array([[best[0], best[1], best_eval]])\n",
    "\n",
    "    # Generaciones\n",
    "    for gen in range(n_iter):\n",
    "        \n",
    "        # Se evaluan los miembros de la población\n",
    "        scores = [fitness(d[0], d[1]) for d in pop]\n",
    "        \n",
    "        # Se busca la mejor solución\n",
    "        for i in range(n_pop):\n",
    "            if scores[i] < best_eval:\n",
    "                best, best_eval = pop[i], scores[i]\n",
    "                point = [float(best[0]), float(best[1]), float(best_eval)]\n",
    "                history_best_ind = np.concatenate((history_best_ind, [point]))\n",
    "                \n",
    "        #Selección de los padres por torneo binario\n",
    "        selected = [selection(pop, scores, k=2) for _ in range(n_pop)]\n",
    "        \n",
    "        # Creación de la siguiente generación\n",
    "        children = list()\n",
    "        for i in range(0, n_pop, 2):\n",
    "            p1, p2 = selected[i], selected[i+1]\n",
    "            \n",
    "            #crossover\n",
    "            for c in crossover_sbx(p1, p2, nc=1):\n",
    "                # mutación\n",
    "                m = mutation(c, bounds, gen)\n",
    "                \n",
    "                children.append(m)\n",
    "                \n",
    "        # replace population\n",
    "        pop = np.array(children)\n",
    "        \n",
    "    return best, history_best_ind\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2e2f1",
   "metadata": {},
   "source": [
    "En la siguiente celda se crea el algoritmo evolutivo de manera interactiva. Con el menú f se elije la función fitness a usar, con n_iter se se ingresa el número de iteraciones, con n_pop el número de individuos en la poblacón y; giro1 y giro se usan para rotar la gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94417221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6277acc3e74b9fb3f13897ed8dd0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='f', index=1, options=('f1', 'f2'), value='f2'), Text(value='100', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(f=[\"f1\", \"f2\"] ,n_iter = \"100\", n_pop = \"100\", giro1=(-360,360,), giro2=(-360,360))\n",
    "def evolutive(f=\"f2\", n_iter = \"100\", n_pop = \"100\", giro1=-15.0,giro2=38):\n",
    "    #funcion a usar\n",
    "    f_name = f\n",
    "    f = globals()[f]\n",
    "    \n",
    "    #numero de ietraciones\n",
    "    n_iter = int(n_iter)\n",
    "    #numero de individuos en la población\n",
    "    n_pop = int(n_pop)\n",
    "\n",
    "    # Rango de los genes\n",
    "    bounds = np.array([[-500, 500], [-500, 500]])\n",
    "    #Tamaño de los genes\n",
    "    n_bits = 64\n",
    "    \n",
    "    #graph\n",
    "    fig_=plt.figure(figsize=(18,6))\n",
    "    ax=fig_.add_subplot(111, projection='3d')\n",
    "    ax.view_init(giro1,giro2)\n",
    "    ax.set_title(\"Funcion \",fontsize=14,fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    \n",
    "    n_points = 40\n",
    "    x,y = np.meshgrid(np.linspace(bounds[0][0], bounds[0][1], n_points), np.linspace(bounds[1][0], bounds[1][1], n_points))\n",
    "    z=np.array(f(x,y))\n",
    "    ax.plot_surface(x,y,z, color=(0.1, 0.2, 0.5, 0.3))\n",
    "    \n",
    "    #alritmo genetico\n",
    "    best, history_best = genetic_algorithm(f, bounds, n_bits, n_iter, n_pop)\n",
    "\n",
    "    #resultados \n",
    "    ax.scatter3D(history_best[:,0], history_best[:,1], history_best[:,2],s=80 ,color=\"red\")\n",
    "    plt.show()\n",
    "    \n",
    "    #print(history_best.shape)\n",
    "    print(\"Resultados finales\\npoint:\",best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc90eb",
   "metadata": {},
   "source": [
    "En la gráfica de los resultados del algoritmo evolutivo se muestran los individuos que tuvieron el valor mas bajo en la función fitnes durante todas las iteraciones. Si se cambia algun valor de los controles interactivos se hace una corrida del algoritmo evlutivo.\n",
    "\n",
    "Se puede observar que los individuos tienden hacer una busqueda más variada, a diferencia del gradiente descendente el algoritmo evolutivo no depende del punto inicial. Dependiendo cual sea el punto inicial el gradiente descendente se dirigirá al mínimo local más cercano, mientras que en el algoritmo evolutivo debido a las operadores pueden crearse soluciones que se acercen a mínimos locales lejanos a los puntos inciales además, habrá n_pop individuos que intentarán hacercarse a un mínimo local. \n",
    "\n",
    "El inconveniente del algoritmo evolutivo es que no se asegura a que se llegue a un mínimo, en el gradiente descendente descendente sí, como el gradiente negativo esta compuesto por las derivadas parciales de la función entonces siempre nos dirijimos en la dirección del mínimo mas cercano. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cap]",
   "language": "python",
   "name": "conda-env-cap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
